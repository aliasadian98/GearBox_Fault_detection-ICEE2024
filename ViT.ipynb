{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7884/7884 [00:26<00:00, 297.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.set_seed(utils.seed)\n",
    "\n",
    "import dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nets\n",
    "import  deeplearning\n",
    "import  torch\n",
    "import  torch.nn                as  nn          # basic building block for neural networks\n",
    "import  matplotlib.pyplot       as  plt         # for plotting\n",
    "import  numpy                   as  np            \n",
    "from    tqdm                    import  tqdm\n",
    "from    sklearn.metrics         import  accuracy_score                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nets.VisionTransformer(\n",
    "                                    img_size        =   utils.input_image_size,\n",
    "                                    patch_size      =   utils.patch_size,\n",
    "                                    in_chans        =   utils.in_chanel,\n",
    "                                    n_classes       =   utils.num_classes,  \n",
    "                                    embed_dim       =   utils.embed_dim,\n",
    "                                    depth           =   utils.depth,\n",
    "                                    n_heads         =   utils.n_heads,\n",
    "                                    mlp_ratio       =   4.0,\n",
    "                                    qkv_bias        =   True,\n",
    "                                    p               =   utils.p,\n",
    "                                    attn_p          =   utils.attention_p\n",
    "                                ).to(utils.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=utils.learning_rate,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              eps=1e-08,\n",
    "                              weight_decay=utils.weight_decay,)\n",
    "criterion = nn.BCELoss() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immg = next(iter(dataloaders.train_loader))[0][0]\n",
    "\n",
    "# Plot tensors in a 4x4 grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "\n",
    "for i, batch in enumerate(dataloaders.train_loader):\n",
    "    for j, tensor in enumerate(batch):\n",
    "        if i * 16 + j < 16:\n",
    "            # Convert tensor to NumPy array for plotting\n",
    "            image_array = tensor.permute(1, 2, 0).cpu().numpy()  # Assuming CHW format, adjust if needed\n",
    "            axes[i, j].imshow(image_array)\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - iteration : 1: 100%|██████████| 173/173 [01:15<00:00,  2.28it/s, accuracy_train=0.8996, avg_train_loss_till_current_batch=0.1993, loss_batch=0.0028]\n",
      "val - iteration : 1: 100%|██████████| 37/37 [00:06<00:00,  5.78it/s, accuracy_val=0.9738, avg_val_loss_till_current_batch=0.1044, loss_batch=0.0022]\n",
      "Train - iteration : 2: 100%|██████████| 173/173 [01:07<00:00,  2.57it/s, accuracy_train=0.9926, avg_train_loss_till_current_batch=0.0273, loss_batch=0.0005]\n",
      "val - iteration : 2: 100%|██████████| 37/37 [00:06<00:00,  5.92it/s, accuracy_val=0.9890, avg_val_loss_till_current_batch=0.0454, loss_batch=0.0008]\n",
      "Train - iteration : 3: 100%|██████████| 173/173 [01:07<00:00,  2.58it/s, accuracy_train=0.9971, avg_train_loss_till_current_batch=0.0107, loss_batch=0.1940]\n",
      "val - iteration : 3: 100%|██████████| 37/37 [00:06<00:00,  5.95it/s, accuracy_val=0.9890, avg_val_loss_till_current_batch=0.0293, loss_batch=0.0094]\n",
      "Train - iteration : 4: 100%|██████████| 173/173 [01:07<00:00,  2.57it/s, accuracy_train=0.9974, avg_train_loss_till_current_batch=0.0088, loss_batch=0.3395]\n",
      "val - iteration : 4: 100%|██████████| 37/37 [00:06<00:00,  6.12it/s, accuracy_val=0.9179, avg_val_loss_till_current_batch=0.2315, loss_batch=0.3802]\n",
      "Train - iteration : 5: 100%|██████████| 173/173 [01:07<00:00,  2.57it/s, accuracy_train=0.9986, avg_train_loss_till_current_batch=0.0032, loss_batch=0.0001]\n",
      "val - iteration : 5: 100%|██████████| 37/37 [00:06<00:00,  5.60it/s, accuracy_val=0.9890, avg_val_loss_till_current_batch=0.0400, loss_batch=0.0001]\n",
      "Train - iteration : 6: 100%|██████████| 173/173 [01:08<00:00,  2.52it/s, accuracy_train=0.9854, avg_train_loss_till_current_batch=0.0574, loss_batch=0.0010]\n",
      "val - iteration : 6: 100%|██████████| 37/37 [00:05<00:00,  6.20it/s, accuracy_val=0.9704, avg_val_loss_till_current_batch=0.0955, loss_batch=0.0158]\n",
      "Train - iteration : 7: 100%|██████████| 173/173 [01:07<00:00,  2.56it/s, accuracy_train=0.9972, avg_train_loss_till_current_batch=0.0087, loss_batch=0.1174]\n",
      "val - iteration : 7: 100%|██████████| 37/37 [00:06<00:00,  5.49it/s, accuracy_val=0.9890, avg_val_loss_till_current_batch=0.0270, loss_batch=0.0328]\n",
      "Train - iteration : 8: 100%|██████████| 173/173 [01:21<00:00,  2.13it/s, accuracy_train=0.9986, avg_train_loss_till_current_batch=0.0038, loss_batch=0.0002]\n",
      "val - iteration : 8: 100%|██████████| 37/37 [00:08<00:00,  4.36it/s, accuracy_val=0.9907, avg_val_loss_till_current_batch=0.0397, loss_batch=0.1493]\n",
      "Train - iteration : 9: 100%|██████████| 173/173 [02:38<00:00,  1.09it/s, accuracy_train=0.9967, avg_train_loss_till_current_batch=0.0092, loss_batch=0.0004]\n",
      "val - iteration : 9: 100%|██████████| 37/37 [00:27<00:00,  1.35it/s, accuracy_val=0.9806, avg_val_loss_till_current_batch=0.0961, loss_batch=0.0001]\n",
      "Train - iteration : 10: 100%|██████████| 173/173 [02:54<00:00,  1.01s/it, accuracy_train=0.9962, avg_train_loss_till_current_batch=0.0122, loss_batch=0.0002]\n",
      "val - iteration : 10: 100%|██████████| 37/37 [00:23<00:00,  1.56it/s, accuracy_val=0.9831, avg_val_loss_till_current_batch=0.0564, loss_batch=0.0080]\n",
      "Train - iteration : 11: 100%|██████████| 173/173 [02:34<00:00,  1.12it/s, accuracy_train=0.9986, avg_train_loss_till_current_batch=0.0041, loss_batch=0.0001]\n",
      "val - iteration : 11: 100%|██████████| 37/37 [00:23<00:00,  1.61it/s, accuracy_val=0.9856, avg_val_loss_till_current_batch=0.0466, loss_batch=0.0036]\n",
      "Train - iteration : 12: 100%|██████████| 173/173 [02:33<00:00,  1.13it/s, accuracy_train=0.9998, avg_train_loss_till_current_batch=0.0018, loss_batch=0.0001]\n",
      "val - iteration : 12: 100%|██████████| 37/37 [00:21<00:00,  1.71it/s, accuracy_val=0.9899, avg_val_loss_till_current_batch=0.0442, loss_batch=0.0005]\n",
      "Train - iteration : 13: 100%|██████████| 173/173 [02:45<00:00,  1.05it/s, accuracy_train=1.0000, avg_train_loss_till_current_batch=0.0001, loss_batch=0.0000]\n",
      "val - iteration : 13: 100%|██████████| 37/37 [00:22<00:00,  1.63it/s, accuracy_val=0.9890, avg_val_loss_till_current_batch=0.0416, loss_batch=0.0001]\n",
      "Train - iteration : 14: 100%|██████████| 173/173 [02:43<00:00,  1.06it/s, accuracy_train=1.0000, avg_train_loss_till_current_batch=0.0001, loss_batch=0.0000]\n",
      "val - iteration : 14: 100%|██████████| 37/37 [00:20<00:00,  1.82it/s, accuracy_val=0.9882, avg_val_loss_till_current_batch=0.0502, loss_batch=0.0001]\n",
      "Train - iteration : 15: 100%|██████████| 173/173 [02:42<00:00,  1.06it/s, accuracy_train=1.0000, avg_train_loss_till_current_batch=0.0000, loss_batch=0.0000]\n",
      "val - iteration : 15: 100%|██████████| 37/37 [00:21<00:00,  1.69it/s, accuracy_val=0.9882, avg_val_loss_till_current_batch=0.0503, loss_batch=0.0000]\n",
      "100%|██████████| 15/15 [35:16<00:00, 141.09s/it]\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, report = deeplearning.train(\n",
    "                                                test_ealuate    = False,\n",
    "                                                train_loader    = dataloaders.train_loader,\n",
    "                                                val_loader      = dataloaders.val_loader,\n",
    "                                                tets_loader     = None,\n",
    "                                                model       = model,\n",
    "                                                model_name  = f\"n_heads={utils.n_heads}_depth={utils.depth}_learning_rate={utils.learning_rate}_weight_decay={utils.weight_decay}_embed_dim={utils.embed_dim}\",\n",
    "                                                \n",
    "                                                epochs          = utils.num_epochs,#utils.num_epochs,\n",
    "                                                device          = utils.device,\n",
    "                                                load_saved_model= False,\n",
    "                                                ckpt_save_freq  = utils.ckpt_save_freq,\n",
    "                                                ckpt_save_path  = r'./model/checkpoint',\n",
    "                                                ckpt_path       = r'./model/',\n",
    "                                                report_path     = r'./model/',\n",
    "\n",
    "                                                optimizer       = optimizer,\n",
    "                                                criterion       = criterion,\n",
    "\n",
    "                                                lr_schedulerr_setting =60,\n",
    "                                                lr_schedulerr  = None,\n",
    "                                                validation_threshold = 0.98\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(7, 7), stride=(7, 7))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=38, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=38, out_features=19, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=19, out_features=1, bias=True)\n",
       "  )\n",
       "  (activate): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model .load_state_dict(torch.load(r'.\\model\\n_heads=12_depth=1_learning_rate=0.001_weight_decay=0.03_embed_dim=192_valid_acc 0.9906531531531532.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = [0.9603040540540541]\n",
      "Test Loss = [0.2320260503692152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "y_true_test = []\n",
    "y_pred_test = []\n",
    "test_accuracy = []\n",
    "test_losses = []\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.cuda()\n",
    "with torch.inference_mode():\n",
    "        for batch in tqdm(dataloaders.test_loader, desc=f\"testing dataset\", leave=False):\n",
    "                x, y = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = model(x).squeeze(1)\n",
    "                \n",
    "\n",
    "                # Ensure that the data type (dtype) matches the device\n",
    "                y, y_hat = y.to(device), y_hat.to(device, dtype=torch.float)\n",
    "\n",
    "                loss = criterion(y_hat.float(), y.float())\n",
    "\n",
    "                test_loss += loss.detach().cpu().item() / len(dataloaders.test_loader)\n",
    "\n",
    "                # Calculate testing accuracy\n",
    "                y_true_test.extend(y.detach().cpu().numpy())\n",
    "                y_pred_test.extend(torch.round(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_acc = accuracy_score(y_true_test, y_pred_test)\n",
    "        test_accuracy.append(test_acc)\n",
    "        test_losses.append(test_loss)    \n",
    "\n",
    "print(f\"Test Accuracy = {test_accuracy}\")\n",
    "print(f\"Test Loss = {test_losses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Define a dictionary to map class indices to class names\n",
    "# class_names = dataloaders.dataset.classes\n",
    "# class_to_idx = dataloaders.dataset.class_to_idx\n",
    "# idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "# # Function to show random images with labels\n",
    "# def show_random_images(loader, num_images=4):\n",
    "#     data_iter = iter(loader)\n",
    "#     fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "\n",
    "#     for i in range(num_images):\n",
    "#         images, labels = next(data_iter)\n",
    "#         ax = axes[i]\n",
    "\n",
    "#         # Choose a random image from the batch\n",
    "#         idx = np.random.randint(0, len(images))\n",
    "#         image = images[idx].numpy().transpose((1, 2, 0))\n",
    "#         label = class_names[labels[idx].item()]\n",
    "\n",
    "#         ax.imshow(image)\n",
    "#         ax.set_title(label)\n",
    "#         ax.axis('off')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Show random images from the training dataset\n",
    "# show_random_images(dataloaders.train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(7, 7), stride=(7, 7))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=38, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=38, out_features=19, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=19, out_features=1, bias=True)\n",
       "  )\n",
       "  (activate): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nets.VisionTransformer(\n",
    "                                    img_size=224,\n",
    "                                    patch_size=7,\n",
    "                                    in_chans=3,\n",
    "                                    n_classes=2,  # Assuming 2 classes (Healthy and Damaged)\n",
    "                                    embed_dim=384//2,\n",
    "                                    depth=1,\n",
    "                                    n_heads=8,\n",
    "                                    mlp_ratio=4.0,\n",
    "                                    qkv_bias=True,\n",
    "                                    p=0.1,\n",
    "                                    attn_p=0.1\n",
    "                                )\n",
    "model .load_state_dict(torch.load(r'.\\model\\ViT_L1_learning_rate=0.001_weight_decay=0.03_valid_acc 1.0.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = torch.zeros(size=[1,3,224,224])\n",
    "vv[0,:,28:28*2,:]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Create a sample PyTorch tensor (replace this with your own tensor)\n",
    "tensor_data = torch.rand(1, 256, 256)  # Assuming a 3-channel image with dimensions 256x256\n",
    "\n",
    "# Convert PyTorch tensor to a PIL Image\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "# image = tensor_to_pil((vv[0]))\n",
    "image = tensor_to_pil(model.patch_embed(vv))\n",
    "\n",
    "\n",
    "# Save the image (optional)\n",
    "image.save(\"output_image.png\")\n",
    "\n",
    "# Display the image (optional)\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def save_tensor_to_svg(tensor, file_path, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Save a 2D tensor as an SVG file using matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor (torch.Tensor): Input 2D tensor\n",
    "    - file_path (str): Path to save the SVG file\n",
    "    - cmap (str, optional): Colormap for the plot. Default is 'viridis'.\n",
    "    \"\"\"\n",
    "    # Convert tensor to NumPy array\n",
    "    array_data = tensor.detach().cpu().numpy() if torch.is_tensor(tensor) else tensor\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Display the array as an image with the specified colormap\n",
    "    im = ax.imshow(array_data, cmap=cmap)\n",
    "\n",
    "    # # Add colorbar for better interpretation (optional)\n",
    "    # cbar = plt.colorbar(im)\n",
    "    # # Hide colorbar if show_colorbar is False\n",
    "    # if not show_colorbar:\n",
    "    #     plt.colorbar(im).remove()\n",
    "    \n",
    "    # Hide the legend\n",
    "    ax.legend().set_visible(False)\n",
    "\n",
    "    # Hide axis numbers\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    \n",
    "\n",
    "    # Save the figure as an SVG file\n",
    "    plt.savefig(file_path, format='pdf', dpi=1600)\n",
    "\n",
    "    # Close the figure to free up resources\n",
    "    plt.close()\n",
    "\n",
    "# Example usage:\n",
    "# Create a random 2D tensor (replace this with your own tensor)\n",
    "random_tensor = torch.rand((10, 10))\n",
    "\n",
    "# Save the tensor as an SVG file\n",
    "save_tensor_to_svg(model.patch_embed(vv)[0], 'after patch.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn              as      sns; sns.set()\n",
    "import  matplotlib.pyplot   as      plt\n",
    "from    matplotlib          import  font_manager as fm, rcParams\n",
    "\n",
    "model(next(iter(dataloaders.test_loader))[0]).shape\n",
    "\n",
    "Heatmap = sns.heatmap(model.blocks[0].attn.attn[0,0,:10,:10].detach().numpy(),\n",
    "                    annot=False,\n",
    "                    square=True,\n",
    "                    # xticklabels=tokens,\n",
    "                    # yticklabels=tokens,\n",
    "                    cbar=False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(7, 7), stride=(7, 7))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=76, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=76, out_features=38, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=38, out_features=1, bias=True)\n",
       "  )\n",
       "  (activate): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nets.VisionTransformer(\n",
    "                                    img_size=224,\n",
    "                                    patch_size=7,\n",
    "                                    in_chans=3,\n",
    "                                    n_classes=2,  # Assuming 2 classes (Healthy and Damaged)\n",
    "                                    embed_dim=384,\n",
    "                                    depth=1,\n",
    "                                    n_heads=8,\n",
    "                                    mlp_ratio=4.0,\n",
    "                                    qkv_bias=True,\n",
    "                                    p=0.1,\n",
    "                                    attn_p=0.1\n",
    "                                )\n",
    "model .load_state_dict(torch.load(r'.\\model\\ViT_L1_learning_rate=0.001_weight_decay=0.03_valid_acc 1.0.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot the training and testing accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
    "#plt.plot(epochs, test_accuracy, label='Testing Accuracy')\n",
    "plt.xticks(epochs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and testing loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "#plt.plot(epochs, test_losses, label='Testing Loss')\n",
    "plt.xticks(epochs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_hat is: {y_hat}\")\n",
    "print(f\"y is: {y}\")\n",
    "print(f\"y_hat shape: {y_hat.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "input_size=(3,224,224)\n",
    "summary(model,input_size = input_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
